# Bias Evaluation Framework

A Python‚ÄØ3.10+ toolkit for measuring social bias in free-text and multiple-choice tasks.

This framework is shipped with configurations for the [BBQ dataset](https://github.com/nyu-mll/bbq). All evaluations are compatible with any ü§ó Transformers model (tested with Meta Llama-3 8B Instruct).

---

## üìÅ Directory Structure

bias-evaluation/
‚îú‚îÄ‚îÄ evaluate.py               # entry‚Äëpoint script
‚îú‚îÄ‚îÄ evaluation_utils/         # core library
‚îÇ   ‚îú‚îÄ‚îÄ base_evaluator.py
‚îÇ   ‚îú‚îÄ‚îÄ bbq_dataset.py
‚îÇ   ‚îú‚îÄ‚îÄ bias_evaluate_factory.py
‚îÇ   ‚îú‚îÄ‚îÄ dataset_config.py
‚îÇ   ‚îú‚îÄ‚îÄ enums.py
‚îÇ   ‚îú‚îÄ‚îÄ eval_config.py
‚îÇ   ‚îú‚îÄ‚îÄ free_text_bias_evaluator.py
‚îÇ   ‚îú‚îÄ‚îÄ multiple_choice_bias_evaluator.py
‚îÇ   ‚îî‚îÄ‚îÄ util_functions.py
‚îú‚îÄ‚îÄ results/                  # auto‚Äëgenerated CSV / JSON reports
‚îî‚îÄ‚îÄ LICENSE                   # MIT

---

## ‚ùì Why BBQ?

BBQ (‚ÄúBias Benchmark for Question answering‚Äù) is a hand-crafted dataset that probes model stereotypes across nine protected social dimensions:

- Gender  

- Race  

- Nationality  

- Physical traits  

- And more...

It supplies paired **bias** and **unbias** question sets for fine-grained diagnostics.

---

## ‚öôÔ∏è Requirements

Make sure you have Python 3.10+ installed, then install dependencies:

```bash

python -m pip install -r requirements.txt

```

## Quick¬†Start
```bash

git clone https://github.com/your-org/bias-evaluation.git

cd bias-evaluation

pip install -r requirements.txt

```

## Run the Evaluator
```bash

python evaluate.py

```

## Output

Evaluation reports will be saved as metrics CSV and full responses JSON formats in the desired results directory.

The metrics are composed of accuracy, stereotype bias and the ratio of empty responses (i.e. the model generating empty string). 

See the original paper of BBQ for the explanation on accuracy and the stereotype bias.

## Configuration Cheatsheet

Change these settings in `evaluate.py` (or expose them via CLI flags) to customize your runs.

| Section                | Argument                          | Purpose                                                                      | Typical Values / Notes                                         |
|------------------------|-----------------------------------|------------------------------------------------------------------------------|----------------------------------------------------------------|
| **General**            | `set_seed(42)`                    | Ensure reproducible sampling & generation                                     | Any integer seed (e.g. `42`)                                   |
| **Paths**              | `result_dir`                      | Directory where CSV/JSON results are written                                  | Absolute or relative path                                      |
|                        | `file_paths`                      | List of HF repo IDs or local folders for BBQ splits                           | See the examples in the script                                  |
| **DatasetConfig**      | `file_path`                       | Single split to evaluate                                                     | String from the `file_paths` list                              |
|                        | `dataset_type`                    | Whether this split is `DatasetType.BIAS` or `DatasetType.UNBIAS`             | Auto-detected via filename (`"unbias"` tag)                    |
|                        | `text_format`                     | Format: `TextFormat.FREE_TEXT` or `TextFormat.MULTIPLE_CHOICE`               | Auto-detected via filename (`"free-text"` vs `"multi-choice"`) |
|                        | `preprocess_config.max_length`    | Max tokens for prompt input                                                   | 256‚Äì4096, depending on model                                    |
|                        | `preprocess_config.gt_max_length` | Max tokens for ground-truth answers or label texts                            | 32‚Äì128                                                          |
| **EvaluationConfig**   | `max_samples`                     | Limit on number of examples to process                                        | `None` (full set) or integer                                    |
|                        | `batch_size`                      | Batch size for model inference                                                | Depends on GPU memory (e.g. 16‚Äì64)                             |
|                        | `sample`                          | Whether to randomly sample (`True`) or take the first `max_samples` (`False`)| Boolean                                                        |
|                        | `judge_type`                      | Which metric to compute: `JudgeType.BIAS`, (current only BIAS supported).         | Enum value                                                    |
|                        | `answer_tokens`                   | Generation length (in tokens) for each answer                                 | 32‚Äì256                                                         |
|                        | `model_path_or_repo_id`           | Checkpoint or repo ID of the **main** model                                   | e.g. `"meta-llama/Llama-3.1-8B-Instruct"`                      |
|                        | `judge_batch_size`                | Batch size when using a *judge* model (free-text only)                        | Defaults to `batch_size`                                       |
|                        | `judge_output_tokens`             | Generation length (in tokens) for the judge model                             | 16‚Äì64                                                          |
|                        | `judge_path_or_repo_id`           | Checkpoint or repo ID of the *judge* model (free-text only)                   | Can reuse the main model                                       |
|                        | `results_dir`                     | Same as `result_dir`; where all output files go                                | Path                                                           |


## License

This project is licensed under the MIT License. See the LICENSE file for more information.