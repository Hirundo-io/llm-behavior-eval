# Repository Guidelines

## Project Structure & Module Organization
Core library code lives in `llm_behavior_eval/`, with behavior-specific helpers collected under `llm_behavior_eval/evaluation_utils/`. Tests reside in `tests/` and mirror module names (for example `llm_behavior_eval/evaluation_utils/util_functions.py` ↔ `tests/test_util_functions.py`). Documentation sources are in `docs/`, while runnable examples sit in `examples/`. Distribution artifacts generated by releases land in `dist/`; do not edit them manually.

## Build, Test, and Development Commands
- `uv pip install -e .`: install the project locally; append `".[mlflow]"` if you need MLflow integration.
- `pytest`: run the full test suite; pass `-k pattern` to scope to a module while iterating.
- `ruff check .` and `ruff format .`: lint and auto-format the codebase.
- `pyright`: run static type checks (CI uses the same configuration).

## Coding Style & Naming Conventions
Follow PEP 8 defaults with 4-space indentation. Prefer `snake_case` for modules, functions, and variables; reserve `PascalCase` for classes and `UPPER_SNAKE_CASE` for constants. Keep public CLI options descriptive and aligned with existing Typer command names. Let `ruff` fix spacing and import order; avoid disabling rules unless there is a clear justification. Type hints are expected on new public functions—match the patterns in `evaluation_utils/`.

## Testing Guidelines
Add or update `tests/test_*.py` files alongside any new feature. Use `pytest` assertions and fixtures; parametrization keeps dataset scenarios readable. Cover both CLI flows (`llm_behavior_eval/__main__.py`) and factory utilities when behavior changes. Keep simulated model outputs deterministic so runs remain reproducible. When adding evaluation datasets, include at least one regression test that exercises parsing and scoring logic.

## Commit & Pull Request Guidelines
Write imperative, concise commit titles (e.g., `Add hallu evaluator smoke tests`). Squash trivial fixups locally before raising a PR. Each PR should explain behavior changes, note impacts on benchmark outputs, and link to any tracking issue. Attach screenshots or sample CLI output when the change affects user-visible results. Mark configuration-sensitive updates (MLflow, datasets, prompt presets) so reviewers can double-check downstream pipelines.

## Security & Configuration Tips
No secrets should live in the repo. Use environment variables for provider tokens and model paths when invoking `evaluate.py`. Verify that large artifacts stay out of version control—`results/` is excluded, so place experimental outputs there and keep them untracked.
